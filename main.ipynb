{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T12:08:41.119426Z",
     "start_time": "2024-08-22T12:08:41.104096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "Classification task of toxic not toxic\n",
    "'''\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2048, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        exp_term = torch.arange(0, d_model, 2)\n",
    "        div_term = torch.exp(exp_term * (-math.log(10_000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe) #Constant tensor buffer which calls the position encoding for each elemental\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    d_model: word embedding length\n",
    "    n_head: number of attention heads\n",
    "    d_k: word embedding is split across multiple heads. This is their new length\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, d_k, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_k = d_k\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.key = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.query = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.value = nn.Linear(d_model, d_k * n_heads)\n",
    "        \n",
    "        self.fc = nn.Linear(d_k * n_heads, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q = self.query(q)\n",
    "        k = self.key(k)\n",
    "        v = self.value(v)\n",
    "        \n",
    "        N = q.shape[0]\n",
    "        T = q.shape[1]\n",
    "        \n",
    "        q = q.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        attn_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask[:, None, None, :] == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        \n",
    "        A = attn_weights @ v\n",
    "        \n",
    "        A = A.transpose(1, 2)\n",
    "        A = A.contiguous().view(N, T, self.d_k * self.n_heads)\n",
    "        \n",
    "        return self.fc(A)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mha = MultiHeadAttention(d_k, d_model, n_heads)\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.ln1(x + self.mha(x, x, x, mask))\n",
    "        x = self.ln2(x + self.ann(x))\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, d_k, d_model, n_heads, n_layers, n_classes, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len, dropout_prob)\n",
    "        transformer_blocks = [\n",
    "            TransformerBlock(d_k, d_model, n_heads, dropout_prob) for _ in range(n_layers)\n",
    "        ]\n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model, n_classes)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, mask)\n",
    "        x = x[:, 0, :]\n",
    "        x = self.ln(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T12:00:47.865136Z",
     "start_time": "2024-08-22T12:00:46.374881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')"
   ],
   "id": "ccacaf9981e726d8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T12:03:59.384725Z",
     "start_time": "2024-08-22T12:03:51.851654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Tokenization and DataLoader preparation\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], truncation=True)\n",
    "\n",
    "# Tokenize the dataset\n",
    "if os.path.isdir(\"tokenized_dataset\"):\n",
    "    tokenized_ds = load_from_disk(\"tokenized_dataset\")\n",
    "else:\n",
    "    ds = load_dataset(\"glue\", \"sst2\")\n",
    "    tokenized_ds = ds.map(tokenize_function, batched=True)\n",
    "    tokenized_ds.save_to_disk(\"tokenized_dataset\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "tokenized_ds = tokenized_ds.remove_columns(['sentence', 'idx'])\n",
    "tokenized_ds = tokenized_ds.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Define PyTorch DataLoader\n",
    "train_loader = DataLoader(tokenized_ds['train'], batch_size=32, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(tokenized_ds['validation'], batch_size=32, collate_fn=data_collator)"
   ],
   "id": "a770c8ee9f535a24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "473dbfd2eecd4a9e83fbe64b32660e57"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b1d3c95c3234a4abbfac969b2db2943"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/872 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cb3036c91d64fca9c2dffb8f82f83de"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eba8048563294251a2ded2af12ca87e4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T12:04:04.523218Z",
     "start_time": "2024-08-22T12:04:04.517906Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_ds",
   "id": "a1edd48e1486ab8f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T12:07:02.354733Z",
     "start_time": "2024-08-22T12:07:02.347868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for batch in val_loader:\n",
    "    for k, v in batch.items():\n",
    "        print(\"k:\", k, \"v.shape\", v.shape)  \n",
    "    break"
   ],
   "id": "a199a0eee0bd46ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: labels v.shape torch.Size([32])\n",
      "k: input_ids v.shape torch.Size([32, 51])\n",
      "k: attention_mask v.shape torch.Size([32, 51])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T12:07:58.395090Z",
     "start_time": "2024-08-22T12:07:58.319935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained('distilbert-base-cased')\n",
    "config.max_position_embeddings"
   ],
   "id": "9dce59d6fa1a1189",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T12:08:49.276034Z",
     "start_time": "2024-08-22T12:08:49.243970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Encoder(tokenizer.vocab_size, config.max_position_embeddings, 16, 64, 4, 2, 2, 0.1)\n",
    "model.to(device)"
   ],
   "id": "97fa55fbdb45cc83",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (embedding): Embedding(28996, 64)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (mha): MultiHeadAttention(\n",
       "        (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (fc): Linear(in_features=64, out_features=64, bias=True)\n",
       "      )\n",
       "      (ann): Sequential(\n",
       "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "        (3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T12:08:52.878938Z",
     "start_time": "2024-08-22T12:08:52.838832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"you are a winner\"\n",
    "model.eval()\n",
    "\n",
    "# Tokenize the input text\n",
    "encoding = tokenizer(\n",
    "    text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")\n",
    "\n",
    "input_ids = encoding['input_ids'].to(device)\n",
    "attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output = model(input_ids, mask=attention_mask)\n",
    "    _, prediction = torch.max(output, dim=1) # batch, prediction\n",
    "    print(prediction)"
   ],
   "id": "8210c4b8ce9f135d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T12:10:11.313696Z",
     "start_time": "2024-08-22T12:08:56.727115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Configuration\n",
    "epochs = 4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "train_losses = np.zeros(epochs)\n",
    "test_losses = np.zeros(epochs)\n",
    "\n",
    "# Training loop\n",
    "for it in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_loss = 0\n",
    "    n_train = 0\n",
    "\n",
    "    # Training phase\n",
    "    for batch in train_loader:\n",
    "        labels = batch['labels'].to(device)\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, mask=attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and count samples\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        n_train += inputs.size(0)\n",
    "\n",
    "    # Average training loss for this epoch\n",
    "    train_loss /= n_train\n",
    "\n",
    "    # Evaluation phase\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    n_test = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch in val_loader:\n",
    "            labels = batch['labels'].to(device)\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            outputs = model(inputs, mask=attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Accumulate loss and count samples\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            n_test +=inputs.size(0)\n",
    "\n",
    "    # Average test loss for this epoch\n",
    "    test_loss /= n_test\n",
    "\n",
    "    # Record losses\n",
    "    train_losses[it] = train_loss\n",
    "    test_losses[it] = test_loss\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ],
   "id": "5189c61f5008efbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4, Train Loss: 0.6549, Test Loss: 0.5915\n",
      "Epoch 2/4, Train Loss: 0.5750, Test Loss: 0.5352\n",
      "Epoch 3/4, Train Loss: 0.5235, Test Loss: 0.5166\n",
      "Epoch 4/4, Train Loss: 0.4861, Test Loss: 0.5165\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T12:10:27.146536Z",
     "start_time": "2024-08-22T12:10:17.971147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "n_correct = 0.\n",
    "n_total = 0.\n",
    "for batch in train_loader:\n",
    "    labels = batch['labels'].to(device)\n",
    "    inputs = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    outputs = model(inputs, mask=attention_mask)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "    \n",
    "    n_correct += (predictions == labels).sum().item()\n",
    "    n_total += labels.shape[0]\n",
    "train_acc = n_correct / n_total\n",
    "    \n",
    "    \n",
    "n_correct = 0.\n",
    "n_total = 0.\n",
    "for batch in val_loader:\n",
    "    labels = batch['labels'].to(device)\n",
    "    inputs = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    outputs = model(inputs, mask=attention_mask)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "    n_correct += (predictions == labels).sum().item()\n",
    "    n_total += labels.shape[0]\n",
    "    \n",
    "test_acc = n_correct / n_total\n",
    "print(f\"Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}\")"
   ],
   "id": "359ec732f5b9a1a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.8068, Test acc: 0.7557\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2e1361f2d6fcd999"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
