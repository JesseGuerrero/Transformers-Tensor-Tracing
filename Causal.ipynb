{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-22T14:04:30.259929Z",
     "start_time": "2024-08-22T14:04:30.255094Z"
    }
   },
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:04:30.875750Z",
     "start_time": "2024-08-22T14:04:30.860681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2048, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        exp_term = torch.arange(0, d_model, 2)\n",
    "        div_term = torch.exp(exp_term * (-math.log(10_000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe) #Constant tensor buffer which calls the position encoding for each elemental\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    '''\n",
    "    d_model: word embedding length\n",
    "    n_head: number of attention heads\n",
    "    d_k: word embedding is split across multiple heads. This is their new length\n",
    "    '''\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_k = d_k\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.key = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.query = nn.Linear(d_model, d_k * n_heads)\n",
    "        self.value = nn.Linear(d_model, d_k * n_heads)\n",
    "\n",
    "        self.fc = nn.Linear(d_k * n_heads, d_model) # d_model x d_model\n",
    "        \n",
    "        # causal mask\n",
    "        '''\n",
    "        1 0 0 0 0 0 0\n",
    "        1 1 0 0 0 0 0\n",
    "        1 1 1 0 0 0 0\n",
    "        1 1 1 1 0 0 0\n",
    "        1 1 1 1 1 0 0\n",
    "        1 1 1 1 1 1 0\n",
    "        1 1 1 1 1 1 1\n",
    "        The point of causal mask is to remove future tokens from consideration and only consider past tokens.\n",
    "        '''\n",
    "        cm = torch.tril(torch.ones(max_len, max_len))\n",
    "        self.register_buffer('causal_mask', cm.view(1, 1, max_len, max_len))\n",
    "\n",
    "    def forward(self, q, k, v, pad_mask=None): # B x Sequence_Length x E+P\n",
    "        # print(f'Inside Causal Attention Layer: {q.shape}')\n",
    "        q = self.query(q)\n",
    "        k = self.key(k)\n",
    "        v = self.value(v)\n",
    "\n",
    "        N = q.shape[0] # N is batch\n",
    "        T = q.shape[1] # T is seq length\n",
    "\n",
    "        q = q.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(N, T, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        # print(f'After query reshaping: {q.shape}') # B x Head x Seq len x Localized Embedding\n",
    "\n",
    "        attn_scores = q @ k.transpose(-2, -1) / math.sqrt(self.d_k)\n",
    "        if pad_mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(pad_mask[:, None, None, :] == 0, float('-inf'))\n",
    "\n",
    "        attn_scores = attn_scores.masked_fill(self.causal_mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "\n",
    "        A = attn_weights @ v\n",
    "\n",
    "        A = A.transpose(1, 2)\n",
    "        A = A.contiguous().view(N, T, self.d_k * self.n_heads)\n",
    "\n",
    "        # print(f'After attention formula: {A.shape}') # B x Seq Length x Context Embedding\n",
    "        return self.fc(A)\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_k, d_model, n_heads, max_len, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mha = CausalSelfAttention(d_k, d_model, n_heads, max_len)\n",
    "        self.ann = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model * 4, d_model),\n",
    "            nn.Dropout(dropout_prob)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x, pad_mask=None):\n",
    "        x = self.ln1(x + self.mha(x, x, x, pad_mask))\n",
    "        x = self.ln2(x + self.ann(x))\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, d_k, d_model, n_heads, n_layers, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEncoding(d_model, max_len)\n",
    "        transformer_blocks = [TransformerBlock(d_k, d_model, n_heads, max_len, dropout_prob) for _ in range(n_layers)]\n",
    "        self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "        self.ln = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x, pad_mask=None): # -> B x Token_IDs from tokenizer Ex. [150, 2089, 500], Hey there Jesse\n",
    "        # print(f'Before embedding: {x.shape}')\n",
    "        x = self.embedding(x) # -> B x Input_Sequence_Length x Embeddings\n",
    "        # print(f'After embedding: {x.shape}')\n",
    "        x = self.pos_embedding(x) # -> B x Input_Sequence_Length x Embeddings + Position\n",
    "        # print(f'After position embedding: {x.shape}')\n",
    "        for block in self.transformer_blocks: # -> B x Input_Sequence_Length x Context\n",
    "            x = block(x, pad_mask)\n",
    "        # print(f'After transformer block: {x.shape}')\n",
    "        x = self.ln(x)\n",
    "        # print(f'Before final output: {x.shape}')\n",
    "        x = self.fc(x)\n",
    "        # print(f'Output: {x.shape}') # -> B x Output_Length x Vocab (Logits for choice) Along the seq length dimension we have logits which determine the vocab among vocab size using softmax and normalization https://chatgpt.com/share/ffc11de7-acdd-4b01-90e4-ede6b87c9c4a\n",
    "        return x"
   ],
   "id": "1516a1da76998764",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:04:31.997775Z",
     "start_time": "2024-08-22T14:04:31.951373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Decoder(20_000, 1024, 16, 64, 4, 2, 0.1)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "x = np.random.randint(0, 20_000, size=(8, 513)) # B x Token_IDs\n",
    "x_t = torch.tensor(x).to(device)\n",
    "y = model(x_t)\n",
    " # B x Length x Vocab"
   ],
   "id": "f07c97a3bc07cfb5",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:04:34.057846Z",
     "start_time": "2024-08-22T14:04:34.045350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "x = np.random.randint(0, 20_000, size=(8, 512)) # B x Length\n",
    "x_t = torch.tensor(x).to(device)\n",
    "x_t"
   ],
   "id": "a6b9663fb6019ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  999, 15687, 16854,  ..., 14702, 11021, 12990],\n",
       "        [19062, 12131, 14673,  ..., 14615,  9298, 19783],\n",
       "        [ 9381, 15003, 11420,  ...,  6942, 15508, 10841],\n",
       "        ...,\n",
       "        [ 8752,  7256,  4837,  ...,  6467, 12846, 13414],\n",
       "        [12029,  5337,  8725,  ..., 16696,  1545,  8525],\n",
       "        [ 8490,  9230,  3536,  ...,  6557,  4661, 16691]], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:04:37.263359Z",
     "start_time": "2024-08-22T14:04:37.254547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = model(x_t)\n",
    "y.shape # B x Length x Vocab"
   ],
   "id": "c99a6a3140db0b47",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 20000])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:04:41.339910Z",
     "start_time": "2024-08-22T14:04:41.334999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mask = np.ones((8, 512)) # 512 is the context window\n",
    "mask[:, 256:] = 0 # second half of sequence is padding/masked out\n",
    "mask_t = torch.tensor(mask).to(device)"
   ],
   "id": "746c871477aab020",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:04:41.922814Z",
     "start_time": "2024-08-22T14:04:41.915456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = model(x_t, mask_t) # Processes all parts with mask = 1\n",
    "y.shape"
   ],
   "id": "3bc59988a9679e47",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 20000])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:04:46.109163Z",
     "start_time": "2024-08-22T14:04:45.893136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')"
   ],
   "id": "7c358d9b0360641d",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:04:47.976558Z",
     "start_time": "2024-08-22T14:04:47.950344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Tokenization and DataLoader preparation\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence'], truncation=True)\n",
    "\n",
    "# Tokenize the dataset\n",
    "if os.path.isdir(\"tokenized_dataset\"):\n",
    "    tokenized_ds = load_from_disk(\"tokenized_dataset\")\n",
    "else:\n",
    "    ds = load_dataset(\"glue\", \"sst2\")\n",
    "    tokenized_ds = ds.map(tokenize_function, batched=True)\n",
    "    tokenized_ds.save_to_disk(\"tokenized_dataset\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "tokenized_ds = tokenized_ds.remove_columns(['sentence', 'idx', \"label\"])\n",
    "\n",
    "# Define PyTorch DataLoader\n",
    "train_loader = DataLoader(tokenized_ds['train'], batch_size=32, shuffle=True, collate_fn=data_collator)\n",
    "val_loader = DataLoader(tokenized_ds['validation'], batch_size=32, collate_fn=data_collator)"
   ],
   "id": "ab4b3163707895c",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:04:51.689732Z",
     "start_time": "2024-08-22T14:04:51.685361Z"
    }
   },
   "cell_type": "code",
   "source": "tokenized_ds",
   "id": "5649a11923136fa2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:04:54.596582Z",
     "start_time": "2024-08-22T14:04:54.589590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for batch in val_loader:\n",
    "    for k, v in batch.items():\n",
    "        print(\"k:\", k, \"v.shape\", v.shape)\n",
    "    break"
   ],
   "id": "19423d70ef9beb32",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k: input_ids v.shape torch.Size([32, 51])\n",
      "k: attention_mask v.shape torch.Size([32, 51])\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:04:56.772339Z",
     "start_time": "2024-08-22T14:04:56.767332Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.pad_token_id",
   "id": "eebd08cd213c707b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:04:59.364119Z",
     "start_time": "2024-08-22T14:04:59.288309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained('distilbert-base-cased')\n",
    "config.max_position_embeddings"
   ],
   "id": "35f6917a3658b5a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:05:07.127345Z",
     "start_time": "2024-08-22T14:05:05.880486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = Decoder(vocab_size=tokenizer.vocab_size, max_len=config.max_position_embeddings, d_k=16, d_model=64, n_heads=4, n_layers=4, dropout_prob=0.1)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters())"
   ],
   "id": "695d0e57954d9a8b",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:05:09.081811Z",
     "start_time": "2024-08-22T14:05:09.075986Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def train(model, criterion, optimizer, train_loader, epochs): # https://chatgpt.com/share/d4a5d9aa-3b6e-4dd2-b060-1ee2a63ec548\n",
    "    train_losses = np.zeros(epochs)\n",
    "    \n",
    "    for it in range(epochs):\n",
    "        model.train()\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            targets = batch['input_ids'].clone().detach()\n",
    "            targets = torch.roll(targets, shifts=-1, dims=1)\n",
    "            targets[:, -1] = tokenizer.pad_token_id\n",
    "            \n",
    "            outputs = model(batch['input_ids'], batch['attention_mask']) # -> B x Seq Length x Vocab\n",
    "            \n",
    "            loss = criterion(outputs.transpose(2, 1), targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        train_loss = np.mean(train_loss)\n",
    "        train_losses[it] = train_loss\n",
    "        \n",
    "        dt = datetime.now() - t0\n",
    "        print(f'Epoch {it+1}/{epochs}, train loss: {train_loss:.4f}, Duration {dt}')\n",
    "    return train_losses"
   ],
   "id": "6a7d565ec3877eb8",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:11:35.014017Z",
     "start_time": "2024-08-22T14:05:12.085460Z"
    }
   },
   "cell_type": "code",
   "source": "train_losses = train(model, criterion, optimizer, train_loader, epochs=5)",
   "id": "fea7966daead0df5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, train loss: 6.0485, Duration 0:01:27.082201\n",
      "Epoch 2/5, train loss: 5.1886, Duration 0:01:32.026814\n",
      "Epoch 3/5, train loss: 4.8464, Duration 0:01:38.311925\n",
      "Epoch 4/5, train loss: 4.6375, Duration 0:01:35.909977\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[68], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_losses \u001B[38;5;241m=\u001B[39m train(model, criterion, optimizer, train_loader, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m)\n",
      "Cell \u001B[1;32mIn[67], line 18\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, criterion, optimizer, train_loader, epochs)\u001B[0m\n\u001B[0;32m     15\u001B[0m targets \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mroll(targets, shifts\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, dims\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     16\u001B[0m targets[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mpad_token_id\n\u001B[1;32m---> 18\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m'\u001B[39m], batch[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]) \u001B[38;5;66;03m# -> B x Seq Length x Vocab\u001B[39;00m\n\u001B[0;32m     20\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m1\u001B[39m), targets)\n\u001B[0;32m     22\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[54], line 117\u001B[0m, in \u001B[0;36mDecoder.forward\u001B[1;34m(self, x, pad_mask)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;66;03m# print(f'After position embedding: {x.shape}')\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m block \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer_blocks: \u001B[38;5;66;03m# -> B x Input_Sequence_Length x Context\u001B[39;00m\n\u001B[1;32m--> 117\u001B[0m     x \u001B[38;5;241m=\u001B[39m block(x, pad_mask)\n\u001B[0;32m    118\u001B[0m \u001B[38;5;66;03m# print(f'After transformer block: {x.shape}')\u001B[39;00m\n\u001B[0;32m    119\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln(x)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[54], line 95\u001B[0m, in \u001B[0;36mTransformerBlock.forward\u001B[1;34m(self, x, pad_mask)\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, pad_mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m---> 95\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln1(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmha(x, x, x, pad_mask))\n\u001B[0;32m     96\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mln2(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mann(x))\n\u001B[0;32m     97\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\test2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[54], line 73\u001B[0m, in \u001B[0;36mCausalSelfAttention.forward\u001B[1;34m(self, q, k, v, pad_mask)\u001B[0m\n\u001B[0;32m     69\u001B[0m attn_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39msoftmax(attn_scores, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     71\u001B[0m A \u001B[38;5;241m=\u001B[39m attn_weights \u001B[38;5;241m@\u001B[39m v\n\u001B[1;32m---> 73\u001B[0m A \u001B[38;5;241m=\u001B[39m A\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     74\u001B[0m A \u001B[38;5;241m=\u001B[39m A\u001B[38;5;241m.\u001B[39mcontiguous()\u001B[38;5;241m.\u001B[39mview(N, T, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39md_k \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_heads)\n\u001B[0;32m     76\u001B[0m \u001B[38;5;66;03m# print(f'After attention formula: {A.shape}') # B x Seq Length x Context Embedding\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:11:46.889452Z",
     "start_time": "2024-08-22T14:11:46.851955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "for batch in val_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(batch['input_ids'], batch['attention_mask'])\n",
    "    break\n",
    "outputs.shape"
   ],
   "id": "23cf324425418881",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 51, 28996])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:11:47.832775Z",
     "start_time": "2024-08-22T14:11:47.806212Z"
    }
   },
   "cell_type": "code",
   "source": "torch.argmax(outputs, axis=-1)",
   "id": "bfc019702f27c0f9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  170,   112,   188,  ...,   102,   102,   102],\n",
       "        [  170, 10879,  2836,  ...,   102,   102,   102],\n",
       "        [  170,  1103,  1106,  ...,   119,   119,   119],\n",
       "        ...,\n",
       "        [  170,   112,   170,  ...,   102,   102,   102],\n",
       "        [  170,  1106,  1129,  ...,   119,   119,   119],\n",
       "        [  170,   119,   119,  ...,  2944,  2944,   119]], device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:11:57.342309Z",
     "start_time": "2024-08-22T14:11:57.338802Z"
    }
   },
   "cell_type": "code",
   "source": "prediction_ids = torch.argmax(outputs, axis=-1)",
   "id": "df45e11955f78bd6",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:11:58.597576Z",
     "start_time": "2024-08-22T14:11:58.590156Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(prediction_ids[0])",
   "id": "18bc77edfa8e2d46",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"a's a good, a funny, [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] [SEP]\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:12:43.245718Z",
     "start_time": "2024-08-22T14:12:43.239861Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(batch['input_ids'][0])",
   "id": "45274b5d817b6f0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] it's a charming and often affecting journey. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:12:44.770580Z",
     "start_time": "2024-08-22T14:12:44.761266Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(torch.concat((batch['input_ids'][0, :5], prediction_ids[:, 4])))",
   "id": "b8c5f31334d4371",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] it's a good and the are, and, [SEP] t ofzing, film and be - emotionsbloid film [SEP] hourry'[SEP] that, the un good at'to\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:12:48.351750Z",
     "start_time": "2024-08-22T14:12:48.345925Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"it's\"\n",
    "\n",
    "tokenized_prompt = tokenizer(prompt, return_tensors='pt')\n",
    "tokenized_prompt"
   ],
   "id": "d9d3940b92165d34",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1122,  112,  188,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:12:51.084676Z",
     "start_time": "2024-08-22T14:12:51.064069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = model(tokenized_prompt['input_ids'][:,:-1].to(device), tokenized_prompt['attention_mask'][:,:-1].to(device))\n",
    "outputs.shape"
   ],
   "id": "99e4ad744d1b3318",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 28996])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:12:52.844668Z",
     "start_time": "2024-08-22T14:12:52.841131Z"
    }
   },
   "cell_type": "code",
   "source": "prediction_ids = torch.argmax(outputs[:, -1, :], axis=-1)",
   "id": "f453176a29c39b9f",
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:12:54.252179Z",
     "start_time": "2024-08-22T14:12:54.246369Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(prediction_ids[0])",
   "id": "394fc50d84f16736",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:12:56.417760Z",
     "start_time": "2024-08-22T14:12:56.342501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"it's\"\n",
    "\n",
    "tokenized_prompt = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "input_ids = tokenized_prompt['input_ids'][:,:-1].to(device)\n",
    "mask = tokenized_prompt['attention_mask'][:,:-1].to(device)\n",
    "\n",
    "for _ in range(20):\n",
    "    outputs = model(input_ids, mask)\n",
    "    prediction_id = torch.argmax(outputs[:, -1, :], axis=-1)\n",
    "    \n",
    "    input_ids = torch.hstack((input_ids, prediction_id.view(1, 1)))\n",
    "    mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    if prediction_id == tokenizer.sep_token_id:\n",
    "        break"
   ],
   "id": "16ec925bccbb597a",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-22T14:12:57.469848Z",
     "start_time": "2024-08-22T14:12:57.463661Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(prediction_ids[0])",
   "id": "500989d4e66951ec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8a3d1fb1709eb771"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
